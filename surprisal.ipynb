{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f0b6db",
   "metadata": {},
   "source": [
    "# **Surprisal**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc77e7",
   "metadata": {},
   "source": [
    "Some reference:\n",
    "- https://www.kaggle.com/code/smidgin/surprisal-with-gpt-2\n",
    "- https://pypi.org/project/pysurprisal/\n",
    "- https://github.com/aalok-sathe/surprisal\n",
    "- https://github.com/byungdoh/slm_surprisal\n",
    "- https://github.com/byungdoh/llm_surprisal\n",
    "- https://github.com/tmalsburg/llm_surprisal\n",
    "- https://github.com/benedict-krieger/llm-surprisal-rerps\n",
    "\n",
    "Where I lerarned:\n",
    "- https://huggingface.co/learn/llm-course/it/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5013b4",
   "metadata": {},
   "source": [
    "Prima importiamo il modello.\n",
    "\n",
    "- ***clean_up_tokenization_spaces*** = indica al 'tokenizer0 di 'ripulire' gli spazi quando decodifica i token in testo. Corregge artefatti di tokenizzazione come spazi prima della punteggiatura, doppi spazi. Può essere messo _False_ se si vuole analizzare il testo così com'è.\n",
    "\n",
    "- ***from IPython.display import clear_output; clear_output()*** = serve a pulire l’output della cella in un notebook Jupyter/IPython. Lo si usa spesso dopo il caricamento del modello per togliere log, warning, o barre di progresso, lasciando il notebook più “pulito” e leggibile. In uno script Python normale (non notebook) non ha effetto e non è necessario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"GroNLP/gpt2-medium-italian-embeddings\").to(device)\n",
    "toeknizer = AutoTokenizer.from_pretrained(\"GroNLP/gpt2-medium-italian-embeddings\", clean_up_tokenization_space = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5787fe",
   "metadata": {},
   "source": [
    "Calcoliamo le metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as D\n",
    "\n",
    "def get_token_metrics (texts: list[str], model: AutoModelForCausalLM, tokenizer: Autotokenizer, truncation=False) -> dict:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    B = len(texts)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors = \"pt\", padding=True, truncation = truncation, return_length =True).to(device)\n",
    "    torch.cuda.empty_cache() #clear any leftover memory\n",
    "    with torch.no_grad(), torch.amp.autocast(device.type):\n",
    "        outputs = model(input_ids = inputs.input_ids, labels = inputs.input_ids)\n",
    "        logits = outputs.logits[:, :-1].float()\n",
    "\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        next_tokens = inputs.input_idf[:, 1:]\n",
    "        attention_mask = inputs.attention_mask[:, 1:]\n",
    "\n",
    "        token_log_probs = log_probs.gather (-1, next_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        surprisals = -token_log_probs * attention_mask\n",
    "        null_first = torch.full((B, 1), float('nan'), device=device)\n",
    "        surprisals = torch.cat((null_first, surprisals), 1)\n",
    "                \n",
    "        tokens = [\n",
    "        tokenizer.convert_ids_to_tokens(seq[:mask.sum()].tolist())\n",
    "        for seq, mask in zip(inputs.input_ids, inputs.attention_mask)]\n",
    "\n",
    "\n",
    "        assert all(len(s) == n for s, n in zip(tokens, inputs.length))\n",
    "        return {\n",
    "        'tok_str':  tokens,  # list[list[str]] jagged shape [B, Tb<=T]\n",
    "        'tok_surp': surprisals.cpu(),  # tensor[B, T]\n",
    "        \n",
    "        'tok_attn': attention_mask.cpu(),  # tensor[B, T]\n",
    "    \n",
    "        'seq_len':  inputs.length.cpu(),  # tensor[B]\n",
    "        'vocab_size': len(tokenizer),  # int\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
