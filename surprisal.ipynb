{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f0b6db",
   "metadata": {},
   "source": [
    "# **Surprisal**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc77e7",
   "metadata": {},
   "source": [
    "Some reference:\n",
    "- https://www.kaggle.com/code/smidgin/surprisal-with-gpt-2\n",
    "- https://pypi.org/project/pysurprisal/\n",
    "- https://github.com/aalok-sathe/surprisal\n",
    "- https://github.com/byungdoh/slm_surprisal\n",
    "- https://github.com/byungdoh/llm_surprisal\n",
    "- https://github.com/tmalsburg/llm_surprisal\n",
    "- https://github.com/benedict-krieger/llm-surprisal-rerps\n",
    "\n",
    "Where I lerarned:\n",
    "- https://huggingface.co/learn/llm-course/it/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5013b4",
   "metadata": {},
   "source": [
    "Prima importiamo il modello.\n",
    "\n",
    "- ***clean_up_tokenization_spaces*** = indica al 'tokenizer0 di 'ripulire' gli spazi quando decodifica i token in testo. Corregge artefatti di tokenizzazione come spazi prima della punteggiatura, doppi spazi. Può essere messo _False_ se si vuole analizzare il testo così com'è.\n",
    "\n",
    "- ***from IPython.display import clear_output; clear_output()*** = serve a pulire l’output della cella in un notebook Jupyter/IPython. Lo si usa spesso dopo il caricamento del modello per togliere log, warning, o barre di progresso, lasciando il notebook più “pulito” e leggibile. In uno script Python normale (non notebook) non ha effetto e non è necessario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b6e81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "toeknizer = GPT2LMHeadModel.from_pretrained(\"gpt2\", clean_up_tokenization_space = True)\n",
    "from IPython.display import clear_output; clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5787fe",
   "metadata": {},
   "source": [
    "Calcoliamo le metriche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fbace",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as D\n",
    "import ml_utils as mu\n",
    "\n",
    "def get_token_metrics (texts: list[str], model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, truncation=False) -> dict:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tolenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    B = len(texts)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    inputs = tokenizer(texts, return_tensors = \"pt\", padding=True, truncation = truncation, return_length =True).to(device)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
